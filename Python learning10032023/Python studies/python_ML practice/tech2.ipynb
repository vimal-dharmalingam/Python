{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "axes = sns.factorplot( 'ATM_TOTAL_INPRN_NUM','Age', data=x_train, aspect = 2.5, )\n",
    "#\n",
    "plt.boxplot(df1['Gross_Annual_Income'])\n",
    "\n",
    "savefig('foo.pdf')\n",
    "\n",
    "#\n",
    "pd.pivot_table(df6, values=['0_txn_6m','0_txn_123m','0_txn_456m','0_txn_last_m'], index=['TARGET_VAR'], columns=None, aggfunc='mean', \n",
    "                   fill_value=0, margins=False, dropna=True, margins_name='All').unstack().plot.bar(figsize=[9,5], stacked=True)\n",
    "#aggfunc=len,sum,mean,median\n",
    "\n",
    "#\n",
    "# Correction Matrix Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import numpy\n",
    "df=pd.read_csv('C:\\\\Users\\\\tvimal\\\\Desktop\\\\cc_casa.csv')\n",
    "correlations = df.corr()\n",
    "correlations\n",
    "# plot correlation matrix\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = numpy.arange(0,26,1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(names)\n",
    "ax.set_yticklabels(names)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_list=[]\n",
    "num_list=[]\n",
    "    \n",
    "for values in df.columns.tolist():\n",
    "    if df[values].dtype=='object':\n",
    "        obj_list.append(values)\n",
    "    else:\n",
    "        num_list.append(values)\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "df[obj_list]=df[obj_list].apply(LabelEncoder().fit_transform)\n",
    "onehotencoder = OneHotEncoder(categorical_features = \"all\")\n",
    "df = onehotencoder.fit_transform(df).toarray()   \n",
    "###########################################\n",
    "col_list=[]\n",
    "num_list=[]\n",
    "    \n",
    "for feature in x.columns.tolist():\n",
    "    if x[feature].dtype=='object':\n",
    "        col_list.append(feature)\n",
    "    else:\n",
    "        num_list.append(feature)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "x[col_list]=x[col_list].apply(LabelEncoder().fit_transform)\n",
    "y=LabelEncoder().fit_transform(y)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "mode_imputer=Imputer(strategy='most_frequent',axis = 0)\n",
    "x[col_list]=mode_imputer.fit_transform(x[col_list])        \n",
    "####################################\n",
    "#label encoding for categorical variable\n",
    "pd.DataFrame(pd.factorize(df7.values.ravel())[0].reshape(df7.shape),df7.index, df7.columns).head(10)\n",
    "############################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "df=pd.read_csv('C:\\\\Users\\\\tvimal\\\\Desktop\\\\PLDATA\\\\pldata21497.csv')\n",
    "df\n",
    "dfnew=df.replace('00_Missing',np.nan)\n",
    "dfnew=dfnew.isnull().count()\n",
    "dfnew\n",
    "df_cln=dfnew.dropna(axis=0,how='any')\n",
    "df_cln\n",
    "#try chi square test Null hypothesis: they are independent, Alternative hypothesis \n",
    "#is that they are correlated in some way.\n",
    "ct=pd.crosstab(df['FLOW_ACTIVE_AVG_CASA_IN_6M'],df['TIME_SEGMENT'])\n",
    "ct\n",
    "#dff = pd.crosstab(df['TARGET_RENEW'],[df['FLOW_ACTIVE_AVG_CASA_IN_6M']] )\n",
    "#dff\n",
    "import scipy.stats as ss\n",
    "ab=ss.chi2_contingency(dff)\n",
    "ab\n",
    "chi_df=pd.DataFrame(columns=['cat1','cat2','pval','chistat'])\n",
    "chi_df\n",
    "col=df.columns.tolist()\n",
    "col\n",
    "for cat1 in col:\n",
    "    for cat2 in col:\n",
    "        if cat1==cat2:\n",
    "            continue\n",
    "        cv_tab=pd.crosstab(df[cat1],df[cat2])\n",
    "        ch_val=ss.chi2_contingency(cv_tab)\n",
    "        chi_df=chi_df.append({'cat1':cat1,'cat2':cat2,'pval':ch_val[1],'chistat':ch_val[0]},ignore_index=True)\n",
    "#not co related\n",
    "#chi_df=chi_df.append({'cat1':'test','cat2':2,'pval':.7,'chistat':3},ignore_index=True)\n",
    "#chi_df[chi_df.pval>0.05] \n",
    "chi_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "z = np.abs(stats.zscore(df))\n",
    "threshold = 2\n",
    "np.where(z > 2)\n",
    "df_o = df[(z < 2).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After pickle scoring test data from towards datascience end to end predictive modelling\n",
    "def score_new(features,clf):\n",
    "    score = pd.DataFrame(clf.predict_proba(features)[:,1], columns = ['SCORE'])\n",
    "    score['DECILE'] = pd.qcut(score['SCORE'].rank(method = 'first'),10,labels=range(10,0,-1))\n",
    "    score['DECILE'] = score['DECILE'].astype(float)\n",
    "    return(score)\n",
    "\n",
    "scores = score_new(new_score_data,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GROPYBY OPTION FOR CATEGORICAL VALUE\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "df.groupby('DEMO_D_Occupation_Group').TARGET_RENEW.count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1_score.\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning curve between cross val score and taining score\n",
    "skplt.estimators.plot_learning_curve(classifier, x_train,y_train)\n",
    "plt.show()\n",
    "#ks chart in python\n",
    "y_probas = classifier.predict_proba(x_test)\n",
    "skplt.metrics.plot_ks_statistic(y_test, y_probas)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#lift curve\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "skplt.metrics.plot_lift_curve(y_test, y_pred)\n",
    "plt.show()\n",
    "#Gain curve\n",
    "#y_pred = classifier.predict_proba(x_test)\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "skplt.metrics.plot_cumulative_gain(y_test, y_pred)\n",
    "plt.show()\n",
    "#precision and recal curve\n",
    "skplt.metrics.plot_precision_recall_curve(y_test, probs)\n",
    "plt.show()\n",
    "\n",
    "#roc curve with two lines\n",
    "import scikitplot as skplt\n",
    "skplt.metrics.plot_roc (y_test,probs)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickling or python file\n",
    "import pickle\n",
    "decision_tree_pkl_filename = 'decision_tree_classifier_21497.pkl'\n",
    "# Open the file to save as pkl file\n",
    "decision_tree_model_pkl = open(decision_tree_pkl_filename, 'wb')\n",
    "pickle_file=pickle.dump(classifier, decision_tree_model_pkl)\n",
    "# Close the pickle instances\n",
    "decision_tree_model_pkl.close()\n",
    "\n",
    "# Loading the saved decision tree model pickle\n",
    "decision_tree_model_pkl = open(decision_tree_pkl_filename, 'rb')\n",
    "decision_tree_model = pickle.load(decision_tree_model_pkl)\n",
    "print (\"Loaded Decision tree model :: \",classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if predict function didnt give real numbers \n",
    "y_pred=classifier.predict(x_test)\n",
    "y_pred=y_pred.ravel()\n",
    "y_pred=y_pred>0.2\n",
    "#y_pred=y_pred.astype('int')\n",
    "y_pred\n",
    "y_pred=y_pred*1\n",
    "y_pred\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,y_pred)\n",
    "print(cm)\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lift curve& gain curve\n",
    "#predict proba gives only 1d array \n",
    "#conversion of 1d array into 2d array with values of 0 and 1\n",
    "y_pred=classifier.predict_proba(x_test)\n",
    "y_pred=y_pred.ravel()#converts 1d into 2d\n",
    "y_pred\n",
    "out=pd.Series(y_pred)\n",
    "out\n",
    "out2=pd.Series(np.ones(len(out)))\n",
    "out2\n",
    "dd=pd.concat([out, out2], axis=1)\n",
    "dd\n",
    "dd2= dd[1] - dd[0]\n",
    "dd2\n",
    "dd3=pd.concat([dd2,dd],ignore_index=True, axis=1)\n",
    "dd3\n",
    "dd4=dd3.drop(2,axis=1)\n",
    "dd4\n",
    "y_pred=dd4.values\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################Zscore\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "z = np.abs(stats.zscore(x))\n",
    "threshold = 3\n",
    "np.where(z > 3)\n",
    "df_o = df[(z < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########seaborn heatmap\n",
    "df2=df.corr()\n",
    "import seaborn as sns\n",
    "sns.heatmap(df2, vmin=0.1, vmax=0.8, cmap=None, \n",
    "                center=None, robust=False, annot=True, fmt='.2g', annot_kws=None, linewidths=0.9, \n",
    "                linecolor='black', cbar=True, cbar_kws=None, cbar_ax=None, square=False, \n",
    "                xticklabels='auto', yticklabels='auto', mask=None, ax=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########seaborn heatmap\n",
    "import seaborn as sns\n",
    "x=df.iloc[:10,:5].values\n",
    "sns.heatmap(x,annot=True)\n",
    "#xt[column].value_counts().plot(kind='bar', label='churn')titanic = sns.load_dataset(\"titanic\")\n",
    "sns.barplot(x,label='churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################use only one column for hist.\n",
    "import seaborn as sns\n",
    "x1=df.iloc[:,1].values\n",
    "list(x1)\n",
    "plt.hist(x1)\n",
    "x2=df.iloc[:,5].values\n",
    "plt.hist(x1,color='indianred',density=True,alpha=0.9,bins=10)\n",
    "plt.hist(x2,density=True,alpha=0.9,bins=10)\n",
    "sns.jointplot(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputer\n",
    "df[\"BAL_INQ_NUM_M\"]=Imputer(missing_values=\"NaN\", strategy=\"median\").fit_transform(df[[\"BAL_INQ_NUM_M\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Histograms\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "df=pd.read_csv('C:\\\\Users\\\\tvimal\\\\Desktop\\\\PLDATA\\\\pldata21497.csv')\n",
    "df.hist()\n",
    "plt.show()\n",
    "plt.cla()   # Clear axis\n",
    "plt.clf()   # Clear figure\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram used to find a column is contributed to 0 and 1\n",
    "df.hist(column=\"PRODUCT_HOLDING\",by=\"TARGET\",bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('C:\\\\Users\\\\tvimal\\\\Desktop\\\\cc_casa\\\\cc_casa.csv')\n",
    "df1=df[df.IF_CC_TAKEUP !=1]\n",
    "df1\n",
    "df2=df[df.IF_CC_TAKEUP !=0]\n",
    "df2\n",
    "df3=df1.replace(0,np.nan)\n",
    "df3.dropna(axis=0,thresh=25,inplace=True)\n",
    "df4 = pd.concat([df3, df2])\n",
    "df4.index = range(len(df4))\n",
    "from sklearn.utils import shuffle\n",
    "df5=shuffle(df4)\n",
    "df6=shuffle(df5)\n",
    "df7=df6.replace(np.nan,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-02457288a978>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-02457288a978>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    >>> chisqprob(3.84, 1)\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#1to find probability using chisqr value and degreee of freedom\n",
    "from scipy.stats import chisqprob\n",
    ">>> chisqprob(3.84, 1)\n",
    "0.050043521248705189\n",
    "#2\n",
    "1 - stats.chi2.cdf(3.84, 1)\n",
    "0.050043521248705147\n",
    "\n",
    "#3\n",
    "from scipy.stats import chisquare\n",
    ">>> chisquare([16, 18, 16, 14, 12, 12],f_exp=[16, 16, 16, 16, 16, 8])\n",
    "output=Power_divergenceResult(statistic=3.5, pvalue=0.62338762774958223)\n",
    "#to find chisquiare, p value between two columns\n",
    "import scipy\n",
    "scipy.stats.chisquare(survey[\"col1\"].value_counts())\n",
    "scipy.stats.chisquare(survey[\"col2\"].value_counts())\n",
    "cont = pd.crosstab(survey[\"col1\"],survey[\"col2\"])\n",
    "scipy.stats.chi2_contingency(cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm grid search\n",
    "from sklearn.modelselection import grid_searchCV\n",
    "parameters=[{'c':[1,10,100,1000],kernel:['linear']},{'c':[1,10,100,1000],kernel:['rbf'],gamma:[0.5,0.1,0.01,0.001]}]\n",
    "grid_search=GridSearchCV(estimator=classifeir,param_grid=parameters,scoring='accuracy',cv=10,n_jobs=-1)\n",
    "\n",
    "grid_search=grid_search.fit(x_train,y_train)\n",
    "best_accuracy=grid_search.best score_\n",
    "best_parameters=grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
