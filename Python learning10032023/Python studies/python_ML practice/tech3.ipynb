{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "class ChiSquare:\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "        self.p = None #P-Value\n",
    "        self.chi2 = None #Chi Test Statistic\n",
    "        self.dof = None\n",
    "        \n",
    "        self.dfTabular = None\n",
    "        self.dfExpected = None\n",
    "    def _print_chisquare_result(self, colX, alpha):\n",
    "        result = \"\"\n",
    "        if self.p<alpha:\n",
    "            result=\"{0} is IMPORTANT for Prediction\".format(colX)\n",
    "        else:\n",
    "            result=\"{0} is NOT an important predictor. (Discard {0} from model)\".format(colX)\n",
    "\n",
    "        print(result)   \n",
    "    def TestIndependence(self,colX,colY, alpha=0.0001):\n",
    "        X = self.df[colX].astype(str)\n",
    "        Y = self.df[colY].astype(str)\n",
    "        \n",
    "        self.dfObserved = pd.crosstab(Y,X) \n",
    "        chi2, p, dof, expected = stats.chi2_contingency(self.dfObserved.values)\n",
    "        self.p = p\n",
    "        self.chi2 = chi2\n",
    "        self.dof = dof \n",
    "        \n",
    "        self.dfExpected = pd.DataFrame(expected, columns=self.dfObserved.columns, index = self.dfObserved.index)\n",
    "        \n",
    "        self._print_chisquare_result(colX, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize ChiSquare Class\n",
    "cT = ChiSquare(input_data)\n",
    "\n",
    "#Feature Selection\n",
    "testColumns = ['Marital_status','Gross_Annual_Income', 'f_OtherProd', 'outflow avg_3m',\n",
    "       'outflow avg_6m', 'outflow_behav_change', 'inflow avg_3m',\n",
    "       'inflow avg_6m', 'inflow_behav_change', 'Age_Band', 'Gender',\n",
    "       'Occupation_Group', 'Seg_Locality']\n",
    "for var in testColumns:\n",
    "    cT.TestIndependence(colX=var,colY=\"target\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [ 1 if y>=0.5 else 0 for y in y_pred ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  matplotlib.pyplot as plt\n",
    "skplt.metrics.plot_confusion_matrix(backtest_op,y_pred1, normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-156b9ccf21e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m }\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Create a based model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_res\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_res\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;31m# Instantiate the grid search model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [8, 10],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier().fit(x_train_res,y_train_res)\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 10, n_jobs = -1, verbose = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(x_train, y_train)\n",
    "grid_search.best_params_\n",
    "{'bootstrap': True,\n",
    " 'max_depth': 80,\n",
    " 'max_features': 3,\n",
    " 'min_samples_leaf': 5,\n",
    " 'min_samples_split': 12,\n",
    " 'n_estimators': 100}\n",
    "best_grid = grid_search.best_estimator_\n",
    "grid_accuracy = evaluate(best_grid, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "ab=ss.chi2_contingency(x)\n",
    "ab\n",
    "chi_df=pd.DataFrame(columns=['cat1','cat2','pval','chistat'])\n",
    "chi_df\n",
    "col=x.columns.tolist()\n",
    "col\n",
    "ab\n",
    "for cat1 in col:\n",
    "    for cat2 in col:\n",
    "        if cat1==cat2:\n",
    "            continue\n",
    "        cv_tab=pd.crosstab(x[cat1],x[cat2])\n",
    "        ch_val=ss.chi2_contingency(cv_tab)\n",
    "        chi_df=chi_df.append({'cat1':cat1,'cat2':cat2,'pval':ch_val[1],'chistat':ch_val[0]},ignore_index=True)\n",
    "chi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch=chi_df['pval'] >= 0.05\n",
    "chh=ch*1\n",
    "chh.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "collections.Counter(y_train)\n",
    "#after Smote\n",
    "import collections\n",
    "collections.Counter(y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"BAL_INQ_NUM_M\"]=Imputer(missing_values=\"NaN\", strategy=\"median\").fit_transform(df[[\"BAL_INQ_NUM_M\"]])\n",
    "df[\"BAL_INQ_AVG_AMT_M\"]=Imputer(missing_values=\"NaN\", strategy=\"median\").fit_transform(df[[\"BAL_INQ_AVG_AMT_M\"]])\n",
    "df=df.fillna({\"BAL_INQ_BEFEND_AVG_AMT_M\":500,'BAL_INQ_NUM_UB500RM_M':1})\n",
    "num_list=[]\n",
    "obj_list=[]\n",
    "for values in df.columns.tolist():\n",
    "    if df[values].dtype=='object':\n",
    "        obj_list.append(values)\n",
    "    else:\n",
    "        num_list.append(values)\n",
    "df[num_list]=Imputer(missing_values=\"NaN\", strategy=\"mean\").fit_transform(df[num_list])\n",
    "df[obj_list]=df[obj_list].apply(LabelEncoder().fit_transform)\n",
    "df1 = pd.concat([df,output], axis=1, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters=[{'c':[1,10,100,1000],'kernel':['linear']},\n",
    "           {'c':[1,10,100,1000],'kernel':['rbf'],'gamma':[0.1,0.2,0.3,0.4,0.5]}]\n",
    "grid_search=GridSearchCV(estimator=classifier,\n",
    "                        param_grid=parameters,\n",
    "                        scoring='accuracy',\n",
    "                        cv=10)\n",
    "grid_search=grid_search.fit(x_train_sm,y_train_sm)\n",
    "best_accuracy=grid_search.best_score_\n",
    "best_parameters=grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.corr()\n",
    "import seaborn as sns\n",
    "sns.heatmap(df2, vmin=0.1, vmax=0.8, cmap=None, \n",
    "                center=None, robust=False, annot=True, fmt='.2g', annot_kws=None, linewidths=0.9, \n",
    "                linecolor='black', cbar=True, cbar_kws=None, cbar_ax=None, square=False, \n",
    "                xticklabels='auto', yticklabels='auto', mask=None, ax=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=10)\n",
    "grid_search.fit(x_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ decision tree grid ssearch\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "classifier=DecisionTreeClassifier( random_state =0,criterion='entropy')\n",
    "classifier.fit(x_train,y_train)\n",
    "y_pred=classifier.predict(x_test)\n",
    "parameters={'min_samples_split' : range(10,500,20),'max_depth': range(1,20,2)}\n",
    "clf=GridSearchCV(classifier,parameters)\n",
    "clf.fit(x_train,y_train)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "classifier_json = classifier.to_json()\n",
    "with open(\"classifier.json\", \"w\") as json_file:\n",
    "    json_file.write(classifier_json)\n",
    "# serialize weights to HDF5\n",
    "classifier.save_weights(\"classifier.h5\")\n",
    "print(\"model Saved  to this working directory\")  \n",
    "\n",
    "--------------------------------------------------\n",
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('classifier.json', 'r')\n",
    "\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "---------------------------------------------\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"classifier.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "---------------------------------------------------\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(x_train,y_train, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "------------------------------------------------------\n",
    "\n",
    "score = loaded_model.predict_proba(x_test)\n",
    "score=score>0.5\n",
    "score=score*1\n",
    "score.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############train test split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=0)\n",
    "\n",
    "############ random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier=RandomForestClassifier(n_estimators=300, random_state =0,criterion='entropy')\n",
    "classifier.fit(x_train,y_train)\n",
    "y_pred=classifier.predict(x_test)\n",
    "print('random forest =',(metrics.accuracy_score (y_test, y_pred))*100)\n",
    "##########naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier=GaussianNB()\n",
    "classifier.fit(x_train,y_train)\n",
    "y_pred=classifier.predict(x_test)\n",
    "print('naive bayes =',(metrics.accuracy_score (y_test, y_pred))*100)\n",
    "############## logistic regeression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier=LogisticRegression(random_state=0)\n",
    "classifier.fit(x_train,y_train)\n",
    "y_pred=classifier.predict(x_test)\n",
    "from sklearn  import metrics\n",
    "print('logistic Regression =',(metrics.accuracy_score (y_test, y_pred))*100)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,y_pred)\n",
    "print(cm)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "classifier=XGBClassifier()\n",
    "classifier.fit(x_train_res,y_train_res)\n",
    "y_pred=classifier.predict(x_test)\n",
    "from sklearn  import metrics\n",
    "print('xgb =',(metrics.accuracy_score (y_test, y_pred))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pylab as plt\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn  import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "df1=pd.read_csv('C:\\\\Users\\\\tvimal\\\\Desktop\\\\demo1L.csv')\n",
    "df1.columns\n",
    "df2=df1.drop(['CIFNO', 'Cust_Nbr','f_PL','Gross_Annual_Income', 'Applied_PL_PastYr',\n",
    "       'Data_Dt','f_CC_CASA','SCV_Party_ID'],axis=1)\n",
    "df2.head()\n",
    "df8=pd.get_dummies(df2)\n",
    "import re\n",
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "df8.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in df8.columns.values]\n",
    "\n",
    "df8 = df8.reindex_axis(sorted(df8.columns), axis=1)\n",
    "df8.head()\n",
    "x=df8.drop(['Open_PL_Jul17Jun18'],axis=1)\n",
    "y=df8['Open_PL_Jul17Jun18']\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.5,random_state=0)\n",
    "X_train = x_train\n",
    "Y_train = y_train\n",
    "xgb_dmat = xgb.DMatrix(X_train, Y_train)\n",
    " \n",
    "param = {'seed': 1111,\n",
    "         'silent':0,\n",
    "         'subsample':0.6,\n",
    "         'objective':'reg:linear',\n",
    "         'bst:max_depth':7,\n",
    "         'bst:eta':0.1,\n",
    "         'colsample_bytree': 0.8,\n",
    "         'colsample_bylevel':1,\n",
    "         'eval_metric':'mae'}\n",
    " \n",
    "plst = list(param.items())\n",
    "num_round = 3\n",
    " \n",
    "gbm = xgb.train(plst, xgb_dmat, num_round)\n",
    " \n",
    "xgb.plot_importance(gbm, max_num_features = 15).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
