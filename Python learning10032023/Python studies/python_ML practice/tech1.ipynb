{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'schedule'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-3881c694e9a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mschedule\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mjob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"I am doing this job!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'schedule'"
     ]
    }
   ],
   "source": [
    "import schedule\n",
    "import time\n",
    "\n",
    "def job():\n",
    "    print(\"I am doing this job!\")\n",
    "\n",
    "\n",
    "schedule.every().monday.at(\"14:00\").do(job)\n",
    "schedule.every().tuesday.at(\"14:00\").do(job)\n",
    "schedule.every().wednesday.at(\"14:00\").do(job)\n",
    "schedule.every().thursday.at(\"14:00\").do(job)\n",
    "schedule.every().friday.at(\"14:00\").do(job)\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_excel('C:\\\\Users\\\\tvimal\\\\Desktop\\\\PL_MODELS\\\\CASA_BASE\\\\DATA\\\\DD_ADHOC.SA_Model_PL_train_DC_TXN_after-exclusion.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102398, 17)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n",
    "dfs=df[['No_of_Prod_Held_BW', 'f_OtherProd', 'Open_PL_Jul17Jun18',\n",
    "       'casa_outflow_123m', 'outflow avg_3m', 'casa_outflow_456m',\n",
    "       'outflow avg_6m', 'outflow_behav_change', 'casa_inflow_123m',\n",
    "       'inflow avg_3m', 'casa_inflow_456m', 'inflow avg_6m','inflow_behav_change', 'MERCHANT_CNT', 'DC_TXN_CNT', \n",
    "        'DC_TXN_AMT','MERCHANT_CNT_SUCCESS_RATE']]\n",
    "dfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tvimal\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\tvimal\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._where(-key, value, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#dfs = dfs.fillna(dfs.mean())\n",
    "dfs[dfs.isnull()] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Some rows only contain missing values: [     0     14     16 ... 102375 102382 102383]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-e6a09ccfd803>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfill_NaN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImputer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mimputed_DF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfill_NaN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mimputed_DF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mimputed_DF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\imputation.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minvalid_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m             raise ValueError(\"Some rows only contain \"\n\u001b[1;32m--> 351\u001b[1;33m                              \"missing values: %s\" % missing)\n\u001b[0m\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[1;31m# Do actual imputation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Some rows only contain missing values: [     0     14     16 ... 102375 102382 102383]"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "import numpy as np\n",
    "fill_NaN = Imputer(missing_values=np.nan, strategy='mean', axis=1)\n",
    "imputed_DF = pd.DataFrame(fill_NaN.fit_transform(dfs))\n",
    "imputed_DF.columns = DF.columns\n",
    "imputed_DF.index = DF.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No_of_Prod_Held_BW              2.000000\n",
      "f_OtherProd                     1.000000\n",
      "Open_PL_Jul17Jun18              0.000000\n",
      "casa_outflow_123m            9039.230000\n",
      "outflow avg_3m               3013.076667\n",
      "casa_outflow_456m            9551.497500\n",
      "outflow avg_6m               3183.832500\n",
      "outflow_behav_change            1.269633\n",
      "casa_inflow_123m             8757.047500\n",
      "inflow avg_3m                2919.015833\n",
      "casa_inflow_456m             9485.535000\n",
      "inflow avg_6m                3161.845000\n",
      "inflow_behav_change             1.367788\n",
      "MERCHANT_CNT                    0.000000\n",
      "DC_TXN_CNT                      0.000000\n",
      "DC_TXN_AMT                      0.000000\n",
      "MERCHANT_CNT_SUCCESS_RATE       0.000000\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(62664, 17)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sns.boxplot(x=df1['0_txn_bhv_chge_3_6m'])\n",
    "Q1 = dfs.quantile(0.25)\n",
    "Q3 = dfs.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(IQR)\n",
    "#print(dfs < (Q1 - 2 * IQR)) |(dfs > (Q3 + 2 * IQR))\n",
    "df_out = dfs[~((dfs< (Q1 -2 * IQR)) |(dfs > (Q3 + 2 * IQR))).any(axis=1)]\n",
    "df_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data visualization\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "# Algorithms\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Dump the trained decision tree classifier with Pickle\n",
    "logistic_regression_pkl_filename = 'RIB_classifier.pickle'\n",
    "# Open the file to save as pkl file\n",
    "logistic_regression_pkl = open(logistic_regression_pkl_filename, 'wb')\n",
    "pickle.dump(classifier, logistic_regression_pkl)\n",
    "\n",
    "\n",
    "# Loading the saved decision tree model pickle\n",
    "logistic_regression_pkl = open(logistic_regression_pkl_filename, 'rb')\n",
    "classifier = pickle.load(logistic_regression_pkl)\n",
    "classifier\n",
    "\n",
    "\n",
    "\n",
    "############\n",
    "from sklearn.externals import joblib\n",
    "# Save to file in the current working directory\n",
    "joblib_file = \"joblib_model.pkl\"  \n",
    "joblib.dump(classifier, joblib_file)\n",
    "# Load from file\n",
    "joblib_classifier = joblib.load(joblib_file)\n",
    "#scre date= z\n",
    "oup = joblib_classifier.predict(z) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost feature importance\n",
    "from matplotlib import pylab as plt\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn  import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "df1=pd.read_csv('C:\\\\Users\\\\tvimal\\\\Desktop\\\\demo1L.csv')\n",
    "df1.columns\n",
    "df2=df1.drop(['CIFNO', 'Cust_Nbr','f_PL','Gross_Annual_Income','Open_PL_Jul17Jun18', 'Applied_PL_PastYr',\n",
    "       'Data_Dt','f_CC_CASA','SCV_Party_ID'],axis=1)\n",
    "df2.head()\n",
    "df8=pd.get_dummies(df2)\n",
    "import re\n",
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "df8.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in df8.columns.values]\n",
    "\n",
    "df8 = df8.reindex_axis(sorted(df8.columns), axis=1)\n",
    "\n",
    " \n",
    "X_train = df8\n",
    "Y_train = df1['Open_PL_Jul17Jun18']\n",
    "xgb_dmat = xgb.DMatrix(X_train, Y_train)\n",
    " \n",
    "param = {'seed': 1111,\n",
    "         'silent':0,\n",
    "         'subsample':0.6,\n",
    "         'objective':'reg:linear',\n",
    "         'bst:max_depth':7,\n",
    "         'bst:eta':0.1,\n",
    "         'colsample_bytree': 0.8,\n",
    "         'colsample_bylevel':1,\n",
    "         'eval_metric':'mae'}\n",
    " \n",
    "plst = list(param.items())\n",
    "num_round = 3\n",
    " \n",
    "gbm = xgb.train(plst, xgb_dmat, num_round)\n",
    " \n",
    "xgb.plot_importance(gbm, max_num_features = 12).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# elasticnet regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "alpha = 0.1\n",
    "enet = ElasticNet(alpha=alpha, rho=0.7)\n",
    "y_pred_enet = enet.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "df.add(Dense(46, input_dim=46,kernel_regularizer=regularizers.l2(0.01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.model_selection import cross_val_score\n",
    "accuracies=cross_val_score(estimator=classifier, X= x_train,y = y_train,cv=10)\n",
    "accuracies.mean()\n",
    "accuracies.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier=DecisionTreeClassifier(criterion='entropy',random_state=0)\n",
    "classifier.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "x_train=sc.fit_transform(x_train)\n",
    "x_test=sc.transform(x_test)\n",
    "sc_y=StandardScaler()\n",
    "y_train=sc_y.transform(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "mode_imputer=Imputer(strategy='most_frequent',axis = 0)\n",
    "x[col_list]=mode_imputer.fit_transform(x[col_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.30,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier=LogisticRegression(random_state=0)\n",
    "classifier.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn  import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "probs =classifier.predict_proba(x_test)\n",
    "y_pred = probs[:,0]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f'%roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN = cm[0][0]\n",
    "FP = cm[0][1]\n",
    "FN = cm[1][0]\n",
    "TP = cm[1][1]\n",
    "tpr=TP/(TP+FN)\n",
    "tpr\n",
    "TNR = TN/(TN+FP)\n",
    "TNR\n",
    "fpr=1-TNR\n",
    "fpr\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "ACC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# naive bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier=GaussianNB()\n",
    "classifier.fit(x_train,y_train)\n",
    "y_pred=classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier=SVC(kernel='linear',random_state=0)\n",
    "classifier.fit(x_train,y_train)\n",
    "y_pred=classifier.predict(x_test)\n",
    "classifier.probability=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "def build_classifier(optimizer):\n",
    "    classifier=Sequential()\n",
    "    classifier.add(Dense(output_dim=140,init='uniform',activation='relu',input_dim=163))\n",
    "    classifier=Sequential([Dense(output_dim=140,input_dim=163,activation='relu'),Dropout(0.25),\n",
    "                       Dense(output_dim=130,input_dim=140,activation='softmax')])\n",
    "    classifier.add(Dense(output_dim=110,init='uniform',activation='relu'))\n",
    "    classifier.add(Dropout(p=0.1))\n",
    "    classifier.add(Dense(output_dim=190,init='uniform',activation='relu'))\n",
    "    classifier.add(Dropout(p=0.1))\n",
    "    #classifier=Sequential([Dense(output_dim=120,input_dim=140,activation='relu'),Dropout(0.1),\n",
    "                       #Dense(output_dim=100,input_dim=120,activation='softmax')])\n",
    "    classifier.add(Dense(output_dim=70,init='uniform',activation='relu'))\n",
    "    classifier.add(Dropout(p=0.1))\n",
    "    classifier.add(Dense(output_dim=50,init='uniform',activation='relu'))\n",
    "    classifier.add(Dropout(p=0.1))\n",
    "    classifier.add(Dense(output_dim=30,init='uniform',activation='relu'))\n",
    "    classifier.add(Dropout(p=0.1))\n",
    "    classifier.add(Dense(output_dim=10,init='uniform',activation='relu'))\n",
    "    classifier.add(Dropout(p=0.1))\n",
    "    classifier.add(Dense(output_dim=1,init='uniform',activation='sigmoid'))\n",
    "    classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    classifier.fit(x_train, y_train, batch_size = 10,nb_epoch = 4, validation_data = (x_test, y_test))\n",
    "    return classifier\n",
    "classifier=KerasClassifier(build_fn=build_classifier)\n",
    "parameters={'batch_size':[20,30],'nb_epoch':[5,15],'optimizer':['rmsprop ','adam']}\n",
    "grid_search=GridSearchCV(estimator=classifier,param_grid=parameters,scoring='accuracy',cv=10)\n",
    "grid_search=grid_search.fit(x_train,y_train)\n",
    "grid_parameters=grid_search.best_params_\n",
    "best_accuracy=grid_search.best_score_\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernal pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import kernel_pca\n",
    "kpca=kernel_pca(n_components=None,kernel='rbf')\n",
    "x_train=kpca.fit_transform(x_train)\n",
    "x_test=kpca.transform(x_test)\n",
    "explained_variance=kpca.explained_variance_ratio_\n",
    "explained_variance\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda=LDA(n_components=2)\n",
    "x_train=lda.fit_transform(x_train,y_train)\n",
    "x_test=lda.transform(x_test)\n",
    "explained_variance=lda.explained_variance_ratio_\n",
    "explained_variance\n",
    "x_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALISE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as  plt\n",
    "df=pd.read_csv('C:\\\\Users\\\\tvimal\\\\Desktop\\\\doc\\\\visualise.csv')\n",
    "ndf= (df-df.min())/df.std()\n",
    "ndf\n",
    "gp=ndf.plot(x='0',y='10',style='o')\n",
    "plt.scatter(ndf['0'],ndf['10'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_0 = LabelEncoder()\n",
    "X[:, 0] = labelencoder_X_0.fit_transform(X[:, 0])\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "\n",
    "onehotencoder = OneHotEncoder(categorical_features = \"all\")\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "labelencoder_y = LabelEncoder()\n",
    "y= onehotencoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL ACCURACY GRAAPH\n",
    "nb_epochs = 10\n",
    "history = classifier.fit(x_train, y_train, batch_size = 10,\n",
    " nb_epoch = nb_epochs, validation_data = (x_test, y_test))\n",
    "\n",
    "print(\"Test score\", history.history[\"val_loss\"][nb_epochs - 1])\n",
    "print(\"Test acc\", history.history[\"val_acc\"][nb_epochs - 1])\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STRATIFIED SAMPLING\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "sss = StratifiedShuffleSplit(n_splits=3, test_size=0.5, random_state=0)\n",
    "sss.get_n_splits(X, y)\n",
    "print(sss) \n",
    "StratifiedShuffleSplit(n_splits=3, random_state=0)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "X_train, X_test = X[train_index], X[test_index]\n",
    "y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##STRATIFIED cross validation\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=10)\n",
    "for train_index, test_index in sss.split(x, y):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EARLY STOPPING(REGULIRISATION NN)\n",
    "from keras.callbacks import EarlyStopping\n",
    "EarlyStopping(monitor='val_err',patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE SAMPLING\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm=SMOTE(random_state=0,ratio=1.0)\n",
    "X_train_res,y_train_res=sm.fit_sample(X_train,y_train)\n",
    "#data label before smote\n",
    "import collections\n",
    "collections.Counter(y_train)\n",
    "#after Smote\n",
    "import collections\n",
    "collections.Counter(y_train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [[\"this is spam, 'SPAM'\"],[\"this is ham, 'HAM'\"],[\"this is nothing, 'NOTHING'\"]]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False,max_features=1500)\n",
    "cv.fit_transform(corpus).toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of word model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv= CountVectorizer(max_features=1500)\n",
    "x=cv.fit_transform(corpus).toarray()\n",
    "y=df.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of word model hashing vectorizer (faster than count vectorizer)\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "cv= HashingVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "x=cv.fit_transform(corpus).toarray()\n",
    "list(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> vect = TfidfVectorizer(min_df=1)\n",
    ">>> tfidf = vect.fit_transform([\"I'd like an apple\",\n",
    "...                             \"An apple a day keeps the doctor away\",\n",
    "...                             \"Never compare an apple to an orange\",\n",
    "...                             \"I prefer scikit-learn to Orange\"])\n",
    ">>> (tfidf * tfidf.T).A\n",
    "array([[ 1.        ,  0.25082859,  0.39482963,  0.        ],\n",
    "       [ 0.25082859,  1.        ,  0.22057609,  0.        ],\n",
    "       [ 0.39482963,  0.22057609,  1.        ,  0.26264139],\n",
    "       [ 0.        ,  0.        ,  0.26264139,  1.        ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
